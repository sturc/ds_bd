{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classification via Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler, VectorIndexer, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from helpers.helper_functions import translate_to_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"ChurnClustering\")\n",
    "       .getOrCreate())\n",
    "# create a DataFrame using an ifered Schema \n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .option(\"delimiter\", \";\") \\\n",
    "       .csv(inputFile)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preperation\n",
    "splits = df.randomSplit([0.6, 0.4 ], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "   \n",
    "# Transform labels into index\n",
    "labelIndexer = StringIndexer(inputCol=\"LEAVE\", outputCol=\"label\").fit(df)\n",
    "collegeIndexer = StringIndexer().setInputCol(\"COLLEGE\").setOutputCol(\"COLLEGE_NUM\")\n",
    "satIndexer = StringIndexer().setInputCol(\"REPORTED_SATISFACTION\").setOutputCol(\"REPORTED_SATISFACTION_NUM\")\n",
    "usageIndexer = StringIndexer().setInputCol(\"REPORTED_USAGE_LEVEL\").setOutputCol(\"REPORTED_USAGE_LEVEL_NUM\")\n",
    "changeIndexer = StringIndexer().setInputCol(\"CONSIDERING_CHANGE_OF_PLAN\").setOutputCol(\"CONSIDERING_CHANGE_OF_PLAN_NUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build feature vector\n",
    "featureCols = df.columns.copy()\n",
    "featureCols.remove(\"LEAVE\")\n",
    "featureCols.remove(\"COLLEGE\")\n",
    "featureCols.remove(\"REPORTED_SATISFACTION\")\n",
    "featureCols.remove(\"REPORTED_USAGE_LEVEL\")\n",
    "featureCols.remove(\"CONSIDERING_CHANGE_OF_PLAN\")\n",
    "featureCols = featureCols +[\"COLLEGE_NUM\",\"REPORTED_SATISFACTION_NUM\",\"REPORTED_USAGE_LEVEL_NUM\",\"CONSIDERING_CHANGE_OF_PLAN_NUM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler =  VectorAssembler(outputCol=\"features\", inputCols=list(featureCols))\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "predConverter = IndexToString(inputCol=\"prediction\",outputCol=\"predictedLabel\",labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MultilayerPerceptronClassifier(seed=1234, featuresCol=\"scaledFeatures\")\n",
    "   \n",
    "# build network parameters grid\n",
    "\t\t\n",
    "\n",
    "paramGrid =  ParamGridBuilder().addGrid(nn.layers, [[ 11, 10, 5, 2 ]]) \\\n",
    "\t\t\t\t.addGrid(nn.blockSize,  [128 ]) \\\n",
    "                .addGrid(nn.maxIter,[ 100, 1000 ] )\\\n",
    "\t\t\t\t.addGrid(nn.stepSize, [0.003, 0.03, 0.3 ])\\\n",
    "\t\t\t\t.addGrid(nn.tol, [ 0.05, 0.1, 0.2 ]) \\\n",
    "\t\t\t\t.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages= [labelIndexer, collegeIndexer, satIndexer,\n",
    "\t\t\t\tusageIndexer, changeIndexer, assembler, scaler, nn, predConverter ])\n",
    "\n",
    "\n",
    "evaluator =  BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator,estimatorParamMaps=paramGrid,numFolds=10, parallelism=2)\n",
    "\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel.stages[7]\n",
    "print(\"Layers: \" , bestModel.layers)\n",
    "print(bestModel.explainParams())\n",
    "  \n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "predictions.show()\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = \" ,(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ]
}