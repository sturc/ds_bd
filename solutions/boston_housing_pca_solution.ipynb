{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Boston Housing PCA und SVD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import BooleanType\n","from pyspark.ml.feature import StringIndexer, VectorAssembler, PCA, StandardScaler\n","from pyspark.ml.classification import LinearSVC\n","from pyspark.sql.session import SparkSession, Row\n","from pyspark.sql.functions import desc, expr\n","from pyspark.mllib.linalg.distributed import RowMatrix\n","from pyspark.mllib.linalg import Vectors\n","from helpers.helper_functions import translate_to_file_string\n","\n","# for pretty printing\n","def printDf(sprkDF): \n","    newdf = sprkDF.toPandas()\n","    from IPython.display import display, HTML\n","    return HTML(newdf.to_html())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputFile = translate_to_file_string(\"../data/Boston_Housing_Data.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["Spark session creation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark = (SparkSession\n","       .builder\n","       .appName(\"BostonHousingPCA\")\n","       .getOrCreate())"]},{"cell_type":"markdown","metadata":{},"source":["DataFrame creation using an ifered Schema "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = spark.read.option(\"header\", \"true\") \\\n","       .option(\"inferSchema\", \"true\") \\\n","       .option(\"delimiter\", \";\") \\\n","       .csv(inputFile) \\\n","       .withColumn(\"CATBOOL\", expr(\"CAT\").cast(BooleanType()))\n","print(df.printSchema())"]},{"cell_type":"markdown","metadata":{},"source":["Prepare training and test data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["featureCols = df.columns.copy()\n","featureCols.remove(\"MEDV\")\n","featureCols.remove(\"CAT\")\n","featureCols.remove(\"CATBOOL\") \n","print(featureCols)\n","\n","assembler =  VectorAssembler(outputCol=\"features\", inputCols=featureCols)\n","featureSet = assembler.transform(df)\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n","                        withStd=True, withMean=False)\n","\n","# Compute summary statistics by fitting the StandardScaler\n","scalerModel = scaler.fit(featureSet)\n","scaledFeatureSet = scalerModel.transform(featureSet)\n"]},{"cell_type":"markdown","metadata":{},"source":["## PCA Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pcaModel = PCA(inputCol=\"scaledFeatures\",outputCol=\"pcaFeatures\",k=7).fit(scaledFeatureSet)\n","result = pcaModel.transform(scaledFeatureSet).select(\"pcaFeatures\")\n","printDf(result.limit(10))"]},{"cell_type":"markdown","metadata":{},"source":["## SVD"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend"]},"outputs":[],"source":["featureVector = scaledFeatureSet.select(\"scaledFeatures\").rdd.map (lambda row : Vectors.dense(row))\n","mat = RowMatrix(featureVector)\n","svd = mat.computeSVD(7, True, 1.0e-9)\n","U = svd.U # The U factor is a RowMatrix.\n","s = svd.s # The singular values are stored in a local dense vector.\n","V = svd.V \n","collectPartitions = U.rows.collect()\n","print(\"U factor is:\")\n","for  vector in collectPartitions :\n","\t   print(\"\\t\", vector)\n","print(\"Singular values are: \", s)\n","print(\"V factor is:\\n\",V )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5-final"}},"nbformat":4,"nbformat_minor":2}